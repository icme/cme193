{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start_slideshow_at': 'selected', 'theme': 'simple', 'transition': 'zoom'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from traitlets.config.manager import BaseJSONConfigManager\n",
    "# To make this work, replace path with your own:\n",
    "# On the command line, type juypter --paths to see where your nbconfig is stored\n",
    "# Should be in the environment in which you install reveal.js\n",
    "path = \" /Users/Blake/.virtualenvs/cme193/bin/../etc/jupyter\"\n",
    "cm = BaseJSONConfigManager(config_dir=path)\n",
    "cm.update('livereveal', {\n",
    "              'theme': 'simple',\n",
    "              'transition': 'zoom',\n",
    "              'start_slideshow_at': 'selected',\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-4-012b78ccbbe5>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-012b78ccbbe5>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    except Exception as e:\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"custom.css\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML \n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"custom.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a499af69-0fe0-4eb0-b156-5f5396da619d"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CME 193 \n",
    "## Introduction to Scientific Python\n",
    "## Spring 2018\n",
    "\n",
    "<br>\n",
    "\n",
    "## Lecture 8\n",
    "-------------\n",
    "## Recursion, Exceptions, Unit Tests, Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "487809eb-b9d2-4bcc-8942-ca61011c3204"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 8 Contents\n",
    "\n",
    "* Admin\n",
    "* Recursion\n",
    "* Exceptions\n",
    "* Unit Testing\n",
    "* Deep Learning\n",
    "* Quick intro to neural nets\n",
    "* Deep Learning Packages\n",
    "* Tensorflow Basics\n",
    "* Keras Basics\n",
    "* More packages\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Administration\n",
    "\n",
    "* Thank you for your project proposals! Really cool and interesting ideas.\n",
    "* Complete either HW2 or Project by **5/15**.\n",
    "* Exercises also due **5/15**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Project tips and general feedback\n",
    "\n",
    "- If your project involves a dataset, make sure you tackle this step early\n",
    "- HW2 is the benchmark for required deliverables.\n",
    "- If you need to pivot along the way, that is fine, if it's substantial let us know\n",
    "- Have fun and research best practices along the way. You are *not* being graded on how well your model works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Office Hours\n",
    "\n",
    "I will continue to hold office hours over the next two weeks:\n",
    "\n",
    "- 2:00-3:15 Mon/Wed in Huang or class time, you decide!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recursion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recursive function solve problems by reducing them to smaller problems of the same form.\n",
    "\n",
    "This allows recursive functions to call themselves.\n",
    " - New paradigm \n",
    " - Powerful tool \n",
    " - Divide-and-conquer \n",
    " - Beautiful solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## First example\n",
    "\n",
    "Let’s consider a trivial problem:\n",
    "\n",
    "Suppose we want to add two positive numbers ```a``` and ```b```, but we can only add/subtract 1 to any number.\n",
    "\n",
    "How would you write a function to do this without recursion? \n",
    "\n",
    "What control statement(s) would you use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Non-recursive solution\n",
    "def add(a, b): \n",
    "    while b > 0:\n",
    "        a += 1\n",
    "        b -= 1 \n",
    "    return a\n",
    "\n",
    "add(7, 8)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Recursive solution\n",
    "\n",
    "- Simple case: \n",
    " - If ```add(a,b)``` is called with ```b = 0``` just return ```a```\n",
    "- Otherwise, we can return ```1 + add(a, b-1)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Recursive solution\n",
    "# Adding b to a, (if only able to use +1)\n",
    "def add(a, b):\n",
    "    if b == 0:\n",
    "        # base case\n",
    "        return a\n",
    "    # recursive step\n",
    "    return add(a, b-1) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Base case and recursive steps\n",
    "\n",
    "Recursive functions consist of two parts:\n",
    "\n",
    "**Base case**: The base case is the trivial case that can be dealt with easily.\n",
    "\n",
    "**Recursive step**: The recursive step brings us slightly closer (breaks the problem into smaller subproblems) to the base case and\n",
    "calls the function itself again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reversing a list\n",
    "\n",
    "How can we recursively reverse a list? ```([1, 2, 3] → [3, 2, 1])```\n",
    "\n",
    " - If list is empty or has one element, the reverse is itself \n",
    " - Otherwise, reverse elements 2 to n, and append the first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 1]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reverse_list(xs):\n",
    "    if len(xs) <= 1:\n",
    "        return xs\n",
    "    else:\n",
    "        # shift first element to last\n",
    "        return reverse_list(xs[1:]) + [xs[0]]\n",
    "    \n",
    "reverse_list([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Palindromes\n",
    "\n",
    "- A palindrome is a word that reads the same from both ways, such as radar or level.\n",
    "\n",
    "- Let’s write a function that checks whether a given word is a palindrome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The recursive idea\n",
    "\n",
    "Given a word, such as level, we check:\n",
    " - whether the first and last character are the same\n",
    " - whether the string with first and last character removed are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Base case\n",
    "\n",
    "What’s the base case in this case? \n",
    "\n",
    "- The empty string is a palindrome \n",
    "- Any 1 letter string is a palindrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def is_palin(s):\n",
    "    '''returns True iff s is a palindrome'''\n",
    "    if len(s) <= 1:\n",
    "        return True\n",
    "    return s[0] == s[-1] and is_palin(s[1:-1])\n",
    "\n",
    "print(is_palin('cme193'))\n",
    "print(is_palin('racecar'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Another example\n",
    "\n",
    "Write a recursive function that computes $a^b$ for given a and b, where b is an integer. (Do not use ∗∗)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Another example\n",
    "\n",
    "Base case: $b=0$ , $a^b =1$\n",
    "\n",
    "Recursive step: (be careful) there are actually two options, one for if b < 0 and one for if b > 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def power(a,b):\n",
    "    if b == 0:\n",
    "        return 1\n",
    "    elif b > 0:\n",
    "        return a*power(a,b-1)\n",
    "    else:\n",
    "        return (1./a)*power(a,b+1)\n",
    "    \n",
    "power(2,10)\n",
    "power(2, -10) == 1.0/1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: Fibonacci\n",
    "\n",
    "```python\n",
    "fib(0) = 0\n",
    "\n",
    "fib(1) = 1\n",
    "\n",
    "fib(n) = fib(n-1) + fib(n-2)  for n >= 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5702887"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fib(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    f = fib(n-1) + fib(n-2)\n",
    "    return f\n",
    "fib(34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pitfalls\n",
    "\n",
    "Recursion can be very powerful, but there are some pitfalls: \n",
    "- Have to ensure you always reach the base case.\n",
    "\n",
    "- Each successive call of the algorithm must be solving a simpler problem\n",
    "\n",
    "- The number of function calls shouldn’t explode. (see exercises)\n",
    "\n",
    "- An iterative algorithm is always faster due to overhead of function calls. (However, the iterative solution might be much more complex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exceptions\n",
    "### Example\n",
    "\n",
    "Consider a function that takes a filename, and returns the 20 most common words. (This is similar to one of the exercises you could have done.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose we have written a function:\n",
    "\n",
    "```python\n",
    "topkwords(filename, k)\n",
    "```\n",
    "\n",
    "\n",
    "Instead of entering ```filename``` and value of ```k``` in the script, we may also want to run it from the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Parse input from command line\n",
    "\n",
    "The sys module allows us to read the terminal command that started the script:\n",
    "\n",
    "``` python\n",
    "import sys\n",
    "\n",
    "print(sys.argv)```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ```sys.argv```\n",
    "\n",
    "```sys.argv``` holds a list with command line arguments passed to a Python script.\n",
    "\n",
    "Note that ```sys.argv[0]``` will be the name of the python script itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "import sys\n",
    "def topkwords(filename, k):\n",
    "# Returns k most common words in filename\n",
    "    pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filename = sys.argv[1]\n",
    "    k = int(sys.argv[2])\n",
    "    print(topkwords(filename, k))```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Issues\n",
    "\n",
    "- What if the file does not exist?\n",
    "- What if the second argument is not an integer? \n",
    "- What if no command line arguments are supplied?\n",
    "- All result in errors: \n",
    " - ```IOError```\n",
    " - ```ValueError```\n",
    " - ```IndexError```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exception handling\n",
    "\n",
    "What do we want to happen when these errors occur? Should the program simply crash?\n",
    "\n",
    "No, we want it to gracefully handle these\n",
    "- ```IOError```: Tell the user the file does not exist.\n",
    "- ```ValueError```, ```IndexError```: Tell the user what the format of the command line arguments should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Try ... Except\n",
    "\n",
    "- The try clause is executed\n",
    "- If no exception occurs, the except clause is skipped\n",
    "- If an exception occurs, the rest of the try clause is skipped. Then if the exception type is matched, the except clause is executed. Then the code continues after the try statement\n",
    "- If an exception occurs with no match in the except clause, execution is stopped and we get the standard error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "import sys\n",
    "if __name__ == \"__main__\": \n",
    "    try:\n",
    "        filename = sys.argv[1]\n",
    "        k = int(sys.argv[2])\n",
    "        print topkwords(filename, k)\n",
    "    except IOError:\n",
    "        print(\"File does not exist\")\n",
    "    except (ValueError, IndexError):\n",
    "        print(\"Error in command line input\")\n",
    "        print(\"Run as: python wc.py <filename> <k>\")\n",
    "        print(\"where <k> is an integer\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A naked except"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A naked except\n",
    "We can have a naked except that catches any error:\n",
    "```python\n",
    "try:\n",
    "    t = 3.0 / 0.0\n",
    "except:\n",
    "    # handles any error\n",
    "    print('There was some error')\n",
    "```\n",
    "\n",
    "Use this with extreme caution though, as genuine bugs might be impossible to correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Try - Except - Else\n",
    "\n",
    "- Else clause is executed only if there is no exception from the ``` try ``` block.\n",
    "\n",
    "Why? \n",
    " - Avoids catching exception that was not protected E.g. consider f.readlines raising an IOError\n",
    " - simplifies code readibility "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "# from Python docs\n",
    "for arg in sys.argv[1:]:\n",
    "    try:\n",
    "        f = open(arg, 'r')\n",
    "    except IOError:\n",
    "        print('cannot open', arg) \n",
    "    else:\n",
    "        print(arg, 'has', len(f.readlines()), 'lines' f.close())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Raise\n",
    "\n",
    "We can use Raise to raise an exception ourselves.\n",
    "\n",
    "```\n",
    ">>> raise NameError(’Oops’)\n",
    "Traceback (most recent call last):\n",
    "  File \"<stdin>\", line 1, in ?\n",
    "NameError: Oops\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ```finally```\n",
    "\n",
    "The finally statement is always executed before leaving the try statement, whether or not an exception has occured.\n",
    "\n",
    "Useful in case we have to close files, closing network connections etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we are error free\n",
      "Finally clause\n",
      "1.5\n",
      "--------------------------------------------------\n",
      "division by zero\n",
      "Finally clause\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def div(x, y):\n",
    "    res = None\n",
    "    try:\n",
    "        res =  x/y\n",
    "    except Exception as e:\n",
    "        print(e) \n",
    "    else:\n",
    "        print(\"we are error free\")\n",
    "    finally:\n",
    "        print(\"Finally clause\")\n",
    "        return res\n",
    "    \n",
    "print(div(3,2))\n",
    "print('-'*50)\n",
    "print(div(3,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Raising our own excecptions\n",
    "\n",
    "Recall the Rational class we considered a few lectures ago.\n",
    "\n",
    "What if the denominator passed in to the constructor is zero?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Raising our own excecptions\n",
    "\n",
    "```python\n",
    "class Rational:\n",
    "    def __init__(self, p, q=1):\n",
    "        g = gcd(p, q)\n",
    "        self.p = p / g\n",
    "        self.q = q / g\n",
    "```\n",
    "\n",
    "What if ```q == 0```?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Making the necessary change\n",
    "\n",
    "```python\n",
    "class Rational:\n",
    "    def __init__(self, p, q=1):\n",
    "        if q == 0:\n",
    "            raise ZeroDivisionError('denominator is zero')\n",
    "        g = gcd(p, q) \n",
    "        self.p = p / g \n",
    "        self.q = q / g\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unit tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![unit_test](../img/unit_test.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Unit tests: \n",
    "\n",
    "Test individual pieces of code.\n",
    "\n",
    "For example, for factorial function, test\n",
    "```0!= 1``` or ```3! = 6``` etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "![title](../img/test_comp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Test driven development\n",
    "\n",
    "Some write tests before code. Reasons:\n",
    "- Focus on the requirements\n",
    "- Don’t write too much\n",
    "- Safely restructure/optimize code\n",
    "- When collaborating: don’t break other’s code \n",
    "- Faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Test cases\n",
    "\n",
    "How to construct test cases?\n",
    "\n",
    "A test case should answer a single question about the code.\n",
    "\n",
    "A test case should:\n",
    "- Run by itself, no human input required\n",
    "- Determine on its own whether the test has passed or failed \n",
    "- Be separate from other tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What to test?\n",
    "- Known values\n",
    "- Sanity check (for conversion functions for example)\n",
    "- Bad input\n",
    " - Input is too large?\n",
    " - Negative input?\n",
    " - String input when expected an integer?\n",
    "- etc: very dependent on problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ```unittest```\n",
    "\n",
    "A testcase is created by subclassing ```unittest.TestCase```\n",
    "\n",
    "Individual tests are defined with methods whose names start with the letters test. (Allows the test runner to identify the tests)\n",
    "\n",
    "Each test usually calls an assert method to run the test - many assert options.\n",
    "\n",
    "A few different ways to run tests (see documentation). Easiest way is to run ```unittest.main()``` for example if the test script is the main program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# ```assert```\n",
    "\n",
    "We can use a number of methods to check for failures:\n",
    "- assertEqual\n",
    "- assertNotEqual\n",
    "- assertTrue, assertFalse\n",
    "- assertIn\n",
    "- assertRaises \n",
    "- assertAlmostEqual \n",
    "- assertGreater, assertLessEqual\n",
    "- etc. (see Docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "import unittest\n",
    "from my_script import is_palindrome\n",
    "\n",
    "class KnownInput(unittest.TestCase):\n",
    "    knownValues = (('lego', False),\n",
    "                   ('radar', True))\n",
    "\n",
    "    def testKnownValues(self):\n",
    "        for word, palin in self.knownValues:\n",
    "            result = is_palindrome(word)\n",
    "            self.assertEqual(result, palin)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ```unittest```\n",
    "\n",
    "Note, to use the ```unittest``` package inside a Jupyter notebook instead of ```unittest.main()```, use:\n",
    "\n",
    "\n",
    "``` unittest.main(argv=['ignored', '-v'], exit=False)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Alternatives\n",
    "\n",
    "- ```nose2```\n",
    "- ```Pytest```\n",
    "\n",
    "http://nose2.readthedocs.io/en/latest/differences.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pytest\n",
    "\n",
    "```pip install pytest```\n",
    "\n",
    "- Easy testing\n",
    "- Automatically discovers tests\n",
    "- No need to remember all assert functions, keyword assert works for everything\n",
    "- Informative failure results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pytest\n",
    "\n",
    "Test discovery: (basics)\n",
    "\n",
    "- Scans files starting with test_ or ending with _test.py\n",
    "- Run functions starting with test_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example : primes\n",
    "\n",
    "Create two files in a directory:\n",
    "    ```primes.py``` – Implementation \n",
    "    ```test_primes.py``` – Tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "# primes.py\n",
    "# (simplest solution that passes tests)\n",
    "\n",
    "def is_prime(x):\n",
    "    for i in range(2, x):\n",
    "        if x % i == 0: \n",
    "            return False\n",
    "    return True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "\n",
    "# test_primes.py\n",
    "from primes import is_prime \n",
    "def test_is_three_prime():\n",
    "    assert is_prime(3)\n",
    "def test_is_four_prime(): \n",
    "    assert not is_prime(4)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Using ```pytest``` to execute test suite\n",
    "\n",
    "#### By default, it will run all files prefixed with test.\n",
    "\n",
    "Here we pass in the name of our test script:\n",
    "\n",
    "```pytest test_primes.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "from primes import is_prime \n",
    "\n",
    "def test_is_zero_prime():\n",
    "    assert not is_prime(0) \n",
    "def test_is_one_prime():\n",
    "    assert not is_prime(1) \n",
    "def test_is_two_prime():\n",
    "    assert is_prime(2)\n",
    "def test_is_three_prime(): \n",
    "    assert is_prime(3)\n",
    "def test_is_four_prime(): \n",
    "    assert not is_prime(4)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Some more tests\n",
    "\n",
    "- Negative numbers \n",
    "- Non integers\n",
    "- Large prime\n",
    "- List of known primes \n",
    "- List of non-primes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### When all tests pass...\n",
    "\n",
    "- First make sure all tests pass\n",
    "- Then optimize code, making sure nothing breaks\n",
    "\n",
    "Now you can be confident that whatever algorithm you use, it still works as desired!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Writing good tests\n",
    "- Utilize automation and code reuse\n",
    "- Know the type and scope - your module or somebody else’s?\n",
    "- A single test should focus on a single thing\n",
    "- Functional tests must be deterministic\n",
    "- Leave no trace - safe setup and clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Let's take a look at some examples..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "# _DEEP LEARNING_\n",
    " - Who can tell me a difference between classical machine learning and deep learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What is Machine Learning?\n",
    "- Deep learning is a subfield of machine learning\n",
    "- Most machine learning\tmethods\twork well because of human-designed\trepresentations and input features\n",
    "- Classical Machine learning is pretty much optimization, i.e. find the best set of weights to optimize predictions for a given loss function\n",
    "- So what's deep learning? \n",
    "- Well, what does wikipedia say?\n",
    "    > \"Deep learning (also known as deep structured learning or hierarchical learning) is part of a broader family of machine learning methods based on learning data representations, as opposed to task-specific algorithms\" \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## So what does that mean?\n",
    "- Lets give an example \n",
    "- Say I wanted to determine whether an image is of a cat vs. a dog. \n",
    "- In classical machine learning, we would have to define features for the input data:\n",
    "    - the weight of the animal\n",
    "    - does it have whiskers \n",
    "    - does it have ears and are they pointed\n",
    "    - does it have ears and are they not pointed  etc.\n",
    "- In short, we have to define a set of facial features and let the optimization identify which features are more important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What do we do in deep learning?\n",
    "![image](../Data/11-fig/catdog.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "* Neural Networks automatically learns which features are important for classification by applying a series of  nonlinear processing units\n",
    "* When people say \"Deep Learning\" they mean using Deep Neural Network\n",
    "\n",
    "* Main differences:\n",
    "    1. ** Feature engineering **: Need to create custom feature space in classical machine learning. Not necessary in deep learning\n",
    "    2. ** Data dimensions **: When the data is small, Deep Learning algorithms don’t perform that well. Typically, neural network algorithms need a large amount of data to learn patterns. With traditional machine learning, features are handcrafted, and thus tend to exhibit superior performance when data is sparse.\n",
    "    3. ** Approach ** Neural Network Training has an end-to-end approach. In classical machine learning, typically you break the problem down into different parts, solve them individually, and combine them to get the result. In deep learning you train the model end to end (black box)\n",
    "    4. ** Training time **: Deep Neural Networks takes alot longer to train.\n",
    "    5. ** Hardward dependencies **: Deep learning usually takes advantage of GPUs to speed up training. \n",
    "    6. ** Interpretability **: Hard to interpret what's going on inside of a deep neural network. Difficult to interpret why a prediction was made. Millions of parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why use deep learning\n",
    "\n",
    "- Manually designed features are often over-specified\n",
    "- Learned features are adaptable \n",
    "- Deep learning is very flexible \n",
    "- Deep learning can handle supervised and unsupervised tasks\n",
    "- Deep learning is has achieved ** superior ** performance in many tasks problems since 2010 (computer vision, NLP, classification tasks, game-playing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why has it gotten better ?\n",
    "- Explosion in the amount of data\n",
    "- Way faster machines with the advent of more powerful CPUs and GPUs\n",
    "- New models, algorithms,ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Network Basics \n",
    "- Perceptron \n",
    "- Fully Connected Forward Neural Networks\n",
    "- Intuition:\n",
    "    - Neural networks are a model of our brain. Each node, neuron, applies an operation on its inputs and passes its outputs to the next layer. These neurons can be connected into networks to fit more complicated patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Perceptron \n",
    "- Most basic artificial neuron. Developed in 50s and 60s by Frank Rosenblatt\n",
    "- So what is a perceptron. Well its simply a dot product operator + a threshold funtion \n",
    "\n",
    "![image](../Data/11-fig/perception.png)\n",
    "\n",
    "- The inputs $x_1,x_2, x_3$ are multiplied by weights $w_1,w_2, w_3$ to determine their relative importance:\n",
    "\n",
    "> > $output = \\begin{cases} 0 \\mbox{ if } \\sum_{j} w_j x_j \\leq \\mbox{thresh} \\\\ 1 \\mbox{ if } \\sum_{j} w_j x_j > \\mbox{thresh} \\end{cases} $\n",
    "\n",
    "- These weights are learned in training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Activation Neuron\n",
    "\n",
    "- More generally, a neural network takes a vector $x$ and makes predictions by a composition of linear transformations and non-linear activation layers.\n",
    "- Each node computes:\n",
    ">  $output = f(Wx + b)$\n",
    "- and passes the output to the next layer of the network\n",
    "- $f$ is some non-linear activation function, W is a matrix of weights, and b is a vector of biases. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sigmoid Neuron\n",
    "- A sigmoid neuron simply uses the sigmoid function as its activation function:\n",
    "> $output = \\sigma(Wx + b)$ \n",
    "- Where $ \\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "![image](../Data/11-fig/sigmoid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Relu activation functions\n",
    "\n",
    "- Relu: $f(x) = max(x, 0)$\n",
    "    > ![image](../Data/11-fig/relu.png)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Tanh activation functions\n",
    "- Tanh: $f(x) = \\tanh(x)$\n",
    " > ![image](../Data/11-fig/tanh.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Fully connected neural networks\n",
    "- Stack layers of neurons together, using the outputs of the previous layer as inputs to the following layer. When there are many layers, the network is ** _deep_**\n",
    "\n",
    "![image](../Data/11-fig/fullnetwork.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Forward Propation:\n",
    "\n",
    "- given $h^0 = x$, we have \n",
    "> $h^{i +1}= f(W^{i} h^{i}  + b^{i})$\n",
    "- where $h^i$ are the output of the $i$'th hidden layer\n",
    "> $\\hat{y} = f(W^{n-1} h^{n-1} + b^{n-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Backpropagation Basics\n",
    "\n",
    "- Given a training set $\\{(x^{(1)}, y^{(1)}), ... , (x^{(m)}, y^{(m)})\\}$ of m training examples. \n",
    "- We need to define a loss  $L = J(W, b; x, y) = \\sum_i (\\hat{y}_i - y_i)^2$ < mean squared loss\n",
    "- Backpropagation is just gradient descent on the weights\n",
    "> $W_{t+1} = W_t - \\eta \\frac{\\partial L}{\\partial W}\\big|_{W_t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Backpropagation Continued\n",
    "- Intuition: flow the gradients with respect to the loss backward through the network to update the weights\n",
    "- More references on backprop:\n",
    "    - https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d\n",
    "    - http://cs231n.github.io/optimization-2/\n",
    "    - http://web.stanford.edu/class/cs224n/lecture_notes/cs224n-2017-gradient-notes.pdf\n",
    "    \n",
    "- It's beautifully local, every node in the network can right away compute two things:\n",
    "    - Its output value\n",
    "    - its local gradient of the inputs with respect to its output value\n",
    "    - Backpropagation is just repeated chain rule through the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Tensorflow\n",
    "- ``` pip install tensorflow ```\n",
    "- Open source, backend software developed by Google Brain before being released under the Apache 2.0 open source license\n",
    "- Advantages:\n",
    "    - Good Community\n",
    "    - Very fledible. You just define your computation as a data flow graph. Can define it however you like. \n",
    "    - Portable: can run on GPUs\n",
    "    - Creates a Static Computational Graph for fast backpropagation \n",
    "- Negatives:\n",
    "    - It's big and complicated\n",
    "    - Lots going on, easy for beginners to feel overwhelmed\n",
    "    - It creates a static computational graph, so it is at times unflexible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tensorflow Basics\n",
    "- Two steps:\n",
    "    - Building the computational graph.\n",
    "    - Running the computational graph.\n",
    "- computational graph is just a series of tensorflow operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=float32) Tensor(\"Const_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "node1 = tf.constant(3.0, dtype=tf.float32)\n",
    "node2 = tf.constant(4.0)\n",
    "print(node1, node2) # Doesn't actually run the graph just creates it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 4.0, 7.0]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "node3 = tf.add(node1, node2)\n",
    "print(sess.run([node1, node2, node3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Placeholders\n",
    "\n",
    "- Well that wasn't very interesting, this only produces constant result.\n",
    "- Don't we need some way to specify inputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Yes! We can parameterize the graph to accept inputs using placeholders (external inputs)\n",
    "- To declare a placeholder, use\n",
    "``` python\n",
    "tf.placeholder(dtype, shape, name) ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "[3. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32)\n",
    "x2 = tf.placeholder(tf.float32)\n",
    "sub_nodes = x - x2\n",
    "print(sess.run(sub_nodes, {x: 5, x2: 2}))\n",
    "print(sess.run(sub_nodes, {x: [5, 2,3], x2: [2,1,1]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Variables  \n",
    "- The whole point of deep learning was to learn the weights, so how do we make those?\n",
    "- We need to tell Tensorflow that these variables correspond to the weights we need to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To do this use ``` tf.Variable(initial_val , dtype) ```\n",
    "- constants as we have seen above are initialized on creation\n",
    "- variables have to be initialized at run time by running ``` tf.global_variables_initializer() ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.5 1.  1.5 2.  2.5]\n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable([.5], dtype = tf.float32) # define our variables\n",
    "b = tf.Variable([-.5], dtype = tf.float32)\n",
    "x = tf.placeholder(tf.float32) # define our inputs\n",
    "linear_predictor = W*x + b # define our model\n",
    "init = tf.global_variables_initializer() #initialize variables\n",
    "sess.run(init) ## initialize variables '\n",
    "feed_dict = {x: [1,2,3,4,5,6]} # specify inputs\n",
    "print(sess.run(linear_predictor, feed_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Define a loss and train \n",
    "- So we've got a simple model $y = Wx + b$, and now we want to learn the correct weights\n",
    "- To do this we need to define a loss function and an optimizer \n",
    "- Tensorflow provides a large number of  [loss functions](https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/losses)\n",
    "- They also provide a large number of [optimizers](https://www.tensorflow.org/api_guides/python/train)\n",
    "- To train a model you specify the computational graph, define the loss function, set a optimizer, initialize your variables, and run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "## CREATE YOUR MODEL \n",
    "W = tf.Variable([.5], dtype = tf.float32) # define our variables\n",
    "b = tf.Variable([-.5], dtype = tf.float32)\n",
    "# define our inputs\n",
    "x = tf.placeholder(tf.float32) \n",
    "# define our model\n",
    "linear_predictor = W*x + b \n",
    " # define placeholder for y variables \n",
    "y = tf.placeholder(tf.float32)\n",
    "# CREATE YOUR LOSS\n",
    "loss = tf.reduce_sum(tf.square(linear_predictor - y))\n",
    "# SPECIFY YOUR OPTIMIZER\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "## CREATE YOUR MODEL \n",
    "W = tf.Variable([.3], dtype=tf.float32)\n",
    "b = tf.Variable([-.3], dtype=tf.float32)\n",
    "# define our inputs\n",
    "x = tf.placeholder(tf.float32)\n",
    "# define our model\n",
    "linear_model = W * x + b\n",
    " # define placeholder for y variables\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# CREATE YOUR LOSS\n",
    "loss = tf.reduce_sum(tf.square(linear_model - y)) # sum of the squares\n",
    "# SPECIFY YOUR OPTIMIZER\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is: 7.687006586820644e-110\n",
      "W: [-0.9999964] b: [0.9999894] loss: 7.598544e-11\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "import time\n",
    "x_train = [1, 2, 3, 4]\n",
    "y_train = [0, -1, -2, -3]\n",
    "# training loop\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init) # reset values to wrong\n",
    "for i in range(1000):\n",
    "  time.sleep(.001)\n",
    "  _, loss_val = sess.run([train, loss], {x: x_train, y: y_train})\n",
    "  print(\"Loss is: {}\".format(loss_val), end = '\\r')\n",
    "# evaluate training accuracy\n",
    "print()\n",
    "curr_W, curr_b, curr_loss = sess.run([W, b, loss], {x: x_train, y: y_train})\n",
    "print(\"W: %s b: %s loss: %s\"%(curr_W, curr_b, curr_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Tensorflow is very powerful\n",
    "\n",
    "- Please read the [documentation](https://www.tensorflow.org/get_started/) for more examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Keras\n",
    "- ``` pip install keras ```\n",
    "- Advantages:\n",
    "    - Way easier to use\n",
    "    - It is more of a front-end library, unlike Tensorflow which is a back-end library. \n",
    "    - Capable of running on top of other Machine and Deep Learning libraries like Tensorflow, CNTK or Theano.\n",
    "- Disadvantages:\n",
    "    - Relatively opaque implementation\n",
    "    - Harder to create your own new networks\n",
    "    - Less control\n",
    "- Let's make a simple binary classfier using one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobperricone/anaconda/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation \n",
    "import numpy as np\n",
    "\n",
    "# define our training/testing set \n",
    "x_train = np.random.random((1000, 10))\n",
    "y_train = np.random.randint(2, size= (1000,1))\n",
    "x_test = np.random.random((200, 10))\n",
    "y_test = np.random.randint(2, size=(200,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6959 - acc: 0.4840     \n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6905 - acc: 0.5190     \n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6886 - acc: 0.5380     \n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6877 - acc: 0.5460     \n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6868 - acc: 0.5460     \n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6860 - acc: 0.5430     \n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6854 - acc: 0.5450     \n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6849 - acc: 0.5410     \n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6839 - acc: 0.5510     \n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6838 - acc: 0.5470     \n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6830 - acc: 0.5490     \n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6824 - acc: 0.5530     \n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6821 - acc: 0.5630     \n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6814 - acc: 0.5510     \n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6809 - acc: 0.5710     \n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6807 - acc: 0.5790     \n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6800 - acc: 0.5820     \n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6795 - acc: 0.5790     \n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6790 - acc: 0.5750     \n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6788 - acc: 0.5790     \n",
      "128/200 [==================>...........] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Add a dense fully conected feed forward layer with 32 hidden units\n",
    "model.add(Dense(32, input_dim = 10, activation = 'relu'))\n",
    "# hidden layer of size 32\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "# output unit\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "# compile the model specifying the loss\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# fit the model \n",
    "model.fit(x_train, y_train, epochs=20,batch_size=128)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Summary\n",
    "#### Keras :\n",
    "- is a fast, flexible protyping tool\n",
    "- Can be used on top of tensorflow\n",
    "- Not apt for large scale research\n",
    "- Good to test out potential ideas on a dataset\n",
    "\n",
    "#### Tensorflow:\n",
    "- is the standard in deep learning research\n",
    "- very flexible, gpu support, automatic differentiation\n",
    "- statically defined: must declare a computational graph and run it\n",
    "- Nice tensorboard visualization module\n",
    "- Most Deep Learning classes at Stanford use Tensorflow\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## PyTorch\n",
    "\n",
    "- Developed in part by Facebook, Stanford, Nvidia...\n",
    "- Similarly flexible to Tensorflow \n",
    "- Dynamic computational graph (each graph is computed on the fly)\n",
    "- This leads to a more pythonic API\n",
    "- I love it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tensors in Pytorch\n",
    "- Conceptually identical to numpy array\n",
    "- Generic tool for scientific computing, no knowledge of deep learning or computational graphs. \n",
    "- They can utilize GPUs to speed up their computation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Variables in Pytorch\n",
    "\n",
    "- autograd package allows for automatic differentiation of variables. \n",
    "- The forward pass through the network defines the computational graph (nodes are tensors, edges are functions)\n",
    "- PyTorch autograd looks a lot like TensorFlow:\n",
    "    - in both frameworks we define a computational graph, and use automatic differentiation to compute gradients. \n",
    "    - difference between the two is that TensorFlow's computational graphs are static and PyTorch uses dynamic computational graphs.\n",
    "    \n",
    "- In pytorch each forward pass defines a new computational graph. \n",
    "\n",
    "- To create a Variable, wrap Tensors in ```Variable``` objects. This variable then represents a node in the computational graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "N, D_in, H, D_out = 64, 500, 100, 10\n",
    "\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients w.r.t var\n",
    "# during the backward pass.\n",
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad = False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad = False)\n",
    "\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Variables during the backward pass.\n",
    "w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is: [4.962239e-07]]]\n",
      "Final loss is 4.962238904226979e-07\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-6\n",
    "for t in range(10000):\n",
    "  # Forward pass: compute predicted y using operations on Variables;\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "  \n",
    "  # Compute and print loss using operations on Variables.\n",
    "  # Now loss is a Variable of shape (1,) and loss.data is a Tensor of shape\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "\n",
    "\n",
    "  # Use autograd to compute the backward pass. This call will compute the\n",
    "  # gradient of loss with respect to all Variables with requires_grad=True.\n",
    "    loss.backward()\n",
    "\n",
    "  # Update weights using gradient descent; w1.data and w2.data are Tensors,\n",
    "  # w1.grad and w2.grad are Variables and w1.grad.data and w2.grad.data are\n",
    "  # Tensors.\n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data\n",
    "\n",
    "    # Manually zero the gradients after running the backward pass\n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()\n",
    "    print(\"Loss is: {}\".format(loss.data.numpy()), end = '\\r')\n",
    "\n",
    "print()\n",
    "print(\"Final loss is {}\".format(loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## That's still fairly cumbersome\n",
    "\n",
    "- When building neural networks, arrange the computation into layers, some of which have learnable parameters which will be optimized during learning.\n",
    "- Use the ``` torch.nn ``` package to define your layers\n",
    "- Create custom networks by subclassing the nn.Module\n",
    "- Really clean code!\n",
    "- Just create a class subclassing the nn.Module\n",
    "    - specify layers in the ```__init__``` \n",
    "    - define a forward pass by ```forward(self,x)``` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class TwoLayerNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(D_in, H)\n",
    "        self.layer2 = nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.layer1(x))\n",
    "        out = self.layer2(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Loss is 5.559909199703839e-10\n"
     ]
    }
   ],
   "source": [
    "# N is batch size; D_in is input dimension; H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs, and wrap them in Variables\n",
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. \n",
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(1000):\n",
    "  # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(\"Final Loss is {}\".format(loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# For more examples... \n",
    "- check out [Pytorch Docs](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Examples\n",
    "- Check notebook titled 01_Simple_Linear_Model.ipynb, ported from Hvass-Labs [Tensorflow Tutorials](https://github.com/Hvass-Labs/TensorFlow-Tutorials)\n",
    "- Provide good introduction to the Tensorflow framework and delve into more complicated examples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Attribution\n",
    "This lecture was developed drawing on material from [Pytest](https://semaphoreci.com/community/tutorials/testing-python-applications-with-pytest), [Pytorch Examples](https://github.com/jcjohnson/pytorch-examples), [UFLDL](http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/), [CS224](http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture1.pdf), and the Tensorflow [documentation](https://www.tensorflow.org/get_started/mnist/beginners)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Wrap Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Zen of Python\n",
    "\n",
    "Very easy to write code.\n",
    "\n",
    "A ton of packages already exist to help do most any tasks you like.\n",
    "\n",
    "Once you know basics, very easy to pick up everything else - and a ton of sources as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Feedback\n",
    "\n",
    "Thanks a lot!\n",
    "\n",
    "Hope you enjoyed the class, learned a lot and will continue using Python!\n",
    "\n",
    "Please fill out feedback forms at the end of the quarter - or feel free to let me know any feedback you have.\n",
    "\n",
    "Questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "nbpresent": {
   "slides": {
    "933f0936-8e4d-487a-b1d2-7de7c8056959": {
     "id": "933f0936-8e4d-487a-b1d2-7de7c8056959",
     "prev": null,
     "regions": {
      "29bfbd0a-4b01-4b14-9acf-280d75ad4e02": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": -0.052023118050195334,
        "y": 0.10089331985492428
       },
       "id": "29bfbd0a-4b01-4b14-9acf-280d75ad4e02"
      }
     }
    }
   },
   "themes": {
    "default": "254e2e7f-be4a-4770-9d75-364f6733f9b5",
    "theme": {}
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
